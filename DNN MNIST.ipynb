{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_X = mnist.data/255 #normalized\n",
    "mnist_y = mnist.target\n",
    "\n",
    "mnist_04_X = []\n",
    "mnist_04_y = []\n",
    "\n",
    "for i in range(mnist_X.shape[0]):\n",
    "    if mnist_y[i] >= 0 and mnist_y[i] <= 4: #filtering out digits from 0 to 4 inclusively\n",
    "        mnist_04_X.append(mnist_X[i])\n",
    "        mnist_04_y.append(mnist_y[i])\n",
    "    \n",
    "mnist_04_X = np.array(mnist_04_X)\n",
    "mnist_04_y = np.array(mnist_04_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(mnist_04_X, mnist_04_y, test_size=0.15, shuffle=True)\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.18, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape (24906, 784)\n",
      "train_y.shape (24906,) -----> 0.697 %\n",
      "val_X.shape (5468, 784)\n",
      "val_y.shape (5468,) -----> 0.153 %\n",
      "test_X.shape (5361, 784)\n",
      "test_y.shape (5361,) -----> 0.15 %\n"
     ]
    }
   ],
   "source": [
    "print('train_X.shape',train_X.shape)\n",
    "print('train_y.shape',train_y.shape, \"----->\", round(train_X.shape[0]/mnist_04_X.shape[0], 3), \"%\")\n",
    "print('val_X.shape',val_X.shape)\n",
    "print('val_y.shape',val_y.shape, \"----->\", round(val_X.shape[0]/mnist_04_X.shape[0], 3), \"%\")\n",
    "print('test_X.shape',test_X.shape)\n",
    "print('test_y.shape',test_y.shape, \"----->\", round(test_X.shape[0]/mnist_04_X.shape[0], 3), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = train_X.shape[1] #num of features\n",
    "n_classes = 5 #number of classes to predict\n",
    "num_neurons = 100 #num of neurons in each hidden layer\n",
    "learning_rate = 0.002\n",
    "n_epochs = 200\n",
    "batch_size = 64\n",
    "keep_prob = 0.65\n",
    "max_no_winner = 6000 #if no validation loss improvement within max_no_winner steps, stop learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size):\n",
    "    \n",
    "    idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "    batch_X = X[idx]\n",
    "    batch_y = y[idx]\n",
    "    \n",
    "    return batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def logdir_generate(mode):\n",
    "    root_logdir = \"tf_logs\"\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    logdir = \"{}/{}/run-{}/\".format(root_logdir, mode, now)\n",
    "    \n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None,\n",
    "    'scale': True\n",
    "}\n",
    "\n",
    "with tf.name_scope('DNN'):\n",
    "    with arg_scope([fully_connected], activation_fn=tf.nn.elu, \n",
    "                   weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                   normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                   normalizer_params=bn_params):\n",
    "        X_drop = dropout(X, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden1 = fully_connected(X_drop, num_neurons)\n",
    "        hidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden2 = fully_connected(hidden1_drop, num_neurons)\n",
    "        hidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden3 = fully_connected(hidden2_drop, num_neurons)\n",
    "        hidden3_drop = dropout(hidden3, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden4 = fully_connected(hidden3_drop, num_neurons)\n",
    "        hidden4_drop = dropout(hidden4, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden5 = fully_connected(hidden4_drop, num_neurons)\n",
    "        hidden5_drop = dropout(hidden5, keep_prob, is_training=is_training)\n",
    "        \n",
    "        logits = fully_connected(hidden5_drop, n_classes, activation_fn=None)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentrophy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentrophy, name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "#exporting useful values to Tensorboard\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "file_writer_train = tf.summary.FileWriter(logdir_generate(\"train\"), tf.get_default_graph())\n",
    "file_writer_test = tf.summary.FileWriter(logdir_generate(\"test\"), tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#creating a saver to save our model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TO TRAIN A NEW MODEL!\n",
      "------------------\n",
      "End of training!\n",
      "------------------\n",
      "Training accuracy: 0.998073\n",
      "Validation accuracy: 0.991222\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "no_winner = 0\n",
    "stop_learning = False\n",
    "a = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    n_batches = int(np.ceil(train_X.shape[0] / batch_size))\n",
    "    \n",
    "    print(\"STARTING TO TRAIN A NEW MODEL!\")\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    #getting initial losses and accuracies\n",
    "    summary_loss_train = loss_summary.eval(feed_dict={is_training: False, X: train_X, y: train_y})\n",
    "    summary_loss_test = loss_summary.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "    summary_accuracy_train = accuracy_summary.eval(feed_dict={is_training: False, X: train_X, y: train_y})\n",
    "    summary_accuracy_val = accuracy_summary.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "    file_writer_train.add_summary(summary_loss_train, 0)\n",
    "    file_writer_train.add_summary(summary_accuracy_train, 0)\n",
    "    file_writer_test.add_summary(summary_loss_test, 0)\n",
    "    file_writer_test.add_summary(summary_accuracy_val, 0)\n",
    "    \n",
    "    #early stopping, initializing winner\n",
    "    winner = loss.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            batch_X, batch_y = random_batch(train_X, train_y, batch_size)\n",
    "            sess.run(training_op, feed_dict={is_training: True, X: batch_X, y: batch_y})\n",
    "            step += 1\n",
    "            \n",
    "            #early stopping\n",
    "            if step % 100 == 0:\n",
    "                loss_val = loss.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "                if winner > loss_val:\n",
    "                    winner = loss_val\n",
    "                    a = step\n",
    "                    no_winner = 0\n",
    "                    save_path = saver.save(sess, \"tmp/winner_model\")\n",
    "                else:\n",
    "                    no_winner += 100\n",
    "                if max_no_winner < no_winner:\n",
    "                    stop_learning = True\n",
    "                    break\n",
    "            #-----------------\n",
    "        if stop_learning: #if there was early stopping\n",
    "            break\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            summary_loss_train = loss_summary.eval(feed_dict={is_training: False, is_training: False, X: batch_X, y: batch_y})\n",
    "            summary_loss_test = loss_summary.eval(feed_dict={is_training: False, X: test_X, y: test_y})\n",
    "            summary_accuracy_train = accuracy_summary.eval(feed_dict={is_training: False, X: train_X, y: train_y})\n",
    "            summary_accuracy_val = accuracy_summary.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "            file_writer_train.add_summary(summary_loss_train, step)\n",
    "            file_writer_train.add_summary(summary_accuracy_train, step)\n",
    "            file_writer_test.add_summary(summary_loss_test, step)\n",
    "            file_writer_test.add_summary(summary_accuracy_val, step)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            save_path = saver.save(sess, \"tmp/my_model\")\n",
    "\n",
    "    tr_acc = accuracy.eval(feed_dict={is_training: False, X: train_X, y: train_y})\n",
    "    val_acc = accuracy.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "    \n",
    "print(\"------------------\")\n",
    "print(\"End of training!\")\n",
    "print(\"------------------\")\n",
    "print(\"Training accuracy:\",tr_acc)\n",
    "print(\"Validation accuracy:\",val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024811031"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner #winner model's loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp/winner_model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"tmp/winner_model\")\n",
    "    acc = accuracy.eval(feed_dict={is_training: False, X: val_X, y: val_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99213606"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
