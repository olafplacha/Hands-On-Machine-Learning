{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from tensorflow.contrib.layers import dropout\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_permutation(X,y):\n",
    "    idx = np.random.permutation(X.shape[0])\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_X = mnist.data/255 #normalized\n",
    "mnist_y = mnist.target\n",
    "\n",
    "mnist_59_X = []\n",
    "mnist_59_y = []\n",
    "\n",
    "for i in range(mnist_X.shape[0]):\n",
    "    if mnist_y[i] >= 5: #filtering out digits from 5 to 9 inclusively\n",
    "        mnist_59_X.append(mnist_X[i])\n",
    "        mnist_59_y.append(mnist_y[i])\n",
    "    \n",
    "mnist_59_X = np.array(mnist_59_X)\n",
    "mnist_59_y = np.array(mnist_59_y)\n",
    "\n",
    "mnist_59_X, mnist_59_y = random_permutation(mnist_59_X, mnist_59_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2a0896da198>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3gcDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2a0a741d860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist_59_X[9045,:].reshape(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flag_5, count_5 = False, 0\n",
    "flag_6, count_6 = False, 0\n",
    "flag_7, count_7 = False, 0\n",
    "flag_8, count_8 = False, 0\n",
    "flag_9, count_9 = False, 0\n",
    "\n",
    "\n",
    "small_dataset_X = []\n",
    "small_dataset_y = []\n",
    "\n",
    "size = 170\n",
    "\n",
    "\n",
    "for i in range(mnist_59_X.shape[0]):\n",
    "    \n",
    "    label = mnist_59_y[i]\n",
    "    \n",
    "    if label == 5 and flag_5 == False:\n",
    "        small_dataset_X.append(mnist_59_X[i,:])\n",
    "        small_dataset_y.append(mnist_59_y[i])\n",
    "        count_5 += 1\n",
    "    if label == 6 and flag_6 == False:\n",
    "        small_dataset_X.append(mnist_59_X[i,:])\n",
    "        small_dataset_y.append(mnist_59_y[i])\n",
    "        count_6 += 1\n",
    "    if label == 7 and flag_7 == False:\n",
    "        small_dataset_X.append(mnist_59_X[i,:])\n",
    "        small_dataset_y.append(mnist_59_y[i])\n",
    "        count_7 += 1\n",
    "    if label == 8 and flag_8 == False:\n",
    "        small_dataset_X.append(mnist_59_X[i,:])\n",
    "        small_dataset_y.append(mnist_59_y[i])\n",
    "        count_8 += 1\n",
    "    if label == 9 and flag_9 == False:\n",
    "        small_dataset_X.append(mnist_59_X[i,:])\n",
    "        small_dataset_y.append(mnist_59_y[i])\n",
    "        count_9 += 1\n",
    "        \n",
    "    if count_5 >= size:\n",
    "        flag_5 = True\n",
    "    if count_6 >= size:\n",
    "        flag_6 = True\n",
    "    if count_7 >= size:\n",
    "        flag_7 = True\n",
    "    if count_8 >= size:\n",
    "        flag_8 = True\n",
    "    if count_9 >= size:\n",
    "        flag_9 = True\n",
    "        \n",
    "small_dataset_X = np.array(small_dataset_X)\n",
    "small_dataset_y = np.array(small_dataset_y) - 5 #so that targets are from 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 784)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(small_dataset_X).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def logdir_generate(mode):\n",
    "    root_logdir = \"tf_logs_transferlearning\"\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    logdir = \"{}/{}/run-{}/\".format(root_logdir, mode, now)\n",
    "    \n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size):\n",
    "    \n",
    "    idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "    batch_X = X[idx]\n",
    "    batch_y = y[idx]\n",
    "    \n",
    "    return batch_X, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(small_dataset_X, small_dataset_y, test_size=0.2, shuffle=True, \n",
    "                                                    stratify=small_dataset_y)\n",
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.25, shuffle=True, \n",
    "                                                  stratify=train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 784)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.,  1.,  2.,  3.,  4.]), array([34, 34, 34, 34, 34], dtype=int64))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_y, return_counts=True) #stratified datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = small_dataset_X.shape[1] #num of features\n",
    "n_classes = 5 #number of classes to predict\n",
    "num_neurons = 100 #num of neurons in each hidden layer\n",
    "learning_rate = 0.0035\n",
    "n_epochs = 2000\n",
    "batch_size = 16\n",
    "keep_prob = 0.7\n",
    "max_no_winner = 8000 #if no validation loss improvement within max_no_winner steps, stop learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None,\n",
    "    'scale': True\n",
    "}\n",
    "\n",
    "with tf.name_scope('DNN'):\n",
    "    with arg_scope([fully_connected], activation_fn=tf.nn.elu, \n",
    "                   weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                   normalizer_fn=tf.contrib.layers.batch_norm,\n",
    "                   normalizer_params=bn_params):\n",
    "        X_drop = dropout(X, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden1 = fully_connected(X_drop, num_neurons, scope=\"hidden1\")\n",
    "        hidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden2 = fully_connected(hidden1_drop, num_neurons, scope=\"hidden2\")\n",
    "        hidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden3 = fully_connected(hidden2_drop, num_neurons, scope=\"hidden3\")\n",
    "        hidden3_drop = dropout(hidden3, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden4 = fully_connected(hidden3_drop, num_neurons, scope=\"hidden4\")\n",
    "        hidden4_drop = dropout(hidden4, keep_prob, is_training=is_training)\n",
    "        \n",
    "        hidden5 = fully_connected(hidden4_drop, num_neurons, scope=\"hidden5\")\n",
    "        hidden5_drop = dropout(hidden5, keep_prob, is_training=is_training)\n",
    "        \n",
    "        logits = fully_connected(hidden5_drop, n_classes, activation_fn=None, scope=\"outputs\")\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentrophy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentrophy, name='loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[12345]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "#exporting useful values to Tensorboard\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "accuracy_summary = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "file_writer_train = tf.summary.FileWriter(logdir_generate(\"train\"), tf.get_default_graph())\n",
    "file_writer_test = tf.summary.FileWriter(logdir_generate(\"test\"), tf.get_default_graph())\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[12345]\")\n",
    "reuse_vars_dict = [var for var in reuse_vars]\n",
    "original_saver = tf.train.Saver(reuse_vars_dict)\n",
    "\n",
    "#creating a saver to save our model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TO TRAIN A NEW MODEL!\n",
      "INFO:tensorflow:Restoring parameters from tmp/winner_model\n",
      "------------------\n",
      "End of training!\n",
      "------------------\n",
      "Training accuracy: 1.0\n",
      "Validation accuracy: 0.964706\n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "no_winner = 0\n",
    "stop_learning = False\n",
    "a = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    n_batches = int(np.ceil(train_X.shape[0] / batch_size))\n",
    "    \n",
    "    print(\"STARTING TO TRAIN A NEW MODEL!\")\n",
    "    \n",
    "    sess.run(init)\n",
    "\n",
    "    original_saver.restore(sess,\"tmp/winner_model\")\n",
    "    \n",
    "    #getting initial losses and accuracies\n",
    "    summary_loss_train = loss_summary.eval(feed_dict={is_training: False, X: train_X, y: train_y})\n",
    "    summary_loss_test = loss_summary.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "    summary_accuracy_train = accuracy_summary.eval(feed_dict={is_training: False, X: train_X, y: train_y})\n",
    "    summary_accuracy_val = accuracy_summary.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "    file_writer_train.add_summary(summary_loss_train, 0)\n",
    "    file_writer_train.add_summary(summary_accuracy_train, 0)\n",
    "    file_writer_test.add_summary(summary_loss_test, 0)\n",
    "    file_writer_test.add_summary(summary_accuracy_val, 0)\n",
    "    \n",
    "    #early stopping, initializing winner\n",
    "    winner = loss.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            batch_X, batch_y = random_batch(train_X, train_y, batch_size)\n",
    "            sess.run(training_op, feed_dict={is_training: True, X: batch_X, y: batch_y})\n",
    "            step += 1\n",
    "            \n",
    "            #early stopping\n",
    "            if step % 100 == 0:\n",
    "                loss_val = loss.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "                if winner > loss_val:\n",
    "                    winner = loss_val\n",
    "                    a = step\n",
    "                    no_winner = 0\n",
    "                    save_path = saver.save(sess, \"tmp_transfer/winner_model\")\n",
    "                else:\n",
    "                    no_winner += 100\n",
    "                if max_no_winner < no_winner:\n",
    "                    stop_learning = True\n",
    "                    break\n",
    "            #-----------------\n",
    "        if stop_learning: #if there was early stopping\n",
    "            break\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            summary_loss_train = loss_summary.eval(feed_dict={is_training: False, is_training: False, X: batch_X, y: batch_y})\n",
    "            summary_loss_test = loss_summary.eval(feed_dict={is_training: False, X: test_X, y: test_y})\n",
    "            summary_accuracy_train = accuracy_summary.eval(feed_dict={is_training: False, X: train_X, y: train_y})\n",
    "            summary_accuracy_val = accuracy_summary.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "            file_writer_train.add_summary(summary_loss_train, step)\n",
    "            file_writer_train.add_summary(summary_accuracy_train, step)\n",
    "            file_writer_test.add_summary(summary_loss_test, step)\n",
    "            file_writer_test.add_summary(summary_accuracy_val, step)\n",
    "            \n",
    "        if epoch % 10 == 0:\n",
    "            save_path = saver.save(sess, \"tmp_transfer/my_model\")\n",
    "\n",
    "    tr_acc = accuracy.eval(feed_dict={is_training: False, X: train_X, y: train_y})\n",
    "    val_acc = accuracy.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "    \n",
    "print(\"------------------\")\n",
    "print(\"End of training!\")\n",
    "print(\"------------------\")\n",
    "print(\"Training accuracy:\",tr_acc)\n",
    "print(\"Validation accuracy:\",val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from tmp_transfer/winner_model\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"tmp_transfer/winner_model\")\n",
    "    acc = accuracy.eval(feed_dict={is_training: False, X: val_X, y: val_y})\n",
    "    logit = logits.eval(feed_dict={is_training: False, X: val_X, y: val_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97058821"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "97% val acc without transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/weights:0' shape=(784, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden2/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden2/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden2/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden3/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden3/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden3/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden4/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden4/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden4/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden5/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden5/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden5/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reuse_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'hidden1/weights:0' shape=(784, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden1/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden2/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden2/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden2/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden3/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden3/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden3/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden4/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden4/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden4/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden5/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden5/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'hidden5/BatchNorm/gamma:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'outputs/weights:0' shape=(100, 5) dtype=float32_ref>,\n",
       " <tf.Variable 'outputs/BatchNorm/beta:0' shape=(5,) dtype=float32_ref>,\n",
       " <tf.Variable 'outputs/BatchNorm/gamma:0' shape=(5,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
